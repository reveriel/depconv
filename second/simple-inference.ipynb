{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from google.protobuf import text_format\n",
    "from second.utils import simplevis\n",
    "from second.pytorch.train import build_network\n",
    "from second.protos import pipeline_pb2\n",
    "from second.utils import config_tool\n",
    "from second.pytorch.builder import (box_coder_builder, input_reader_builder,\n",
    "                                    lr_scheduler_builder, optimizer_builder,\n",
    "                                    second_builder)\n",
    "from second.data.preprocess import merge_second_batch, merge_second_batch_multigpu\n",
    "from second.pytorch.train import _worker_init_fn\n",
    "\n",
    "from second.pytorch.train import example_convert_to_torch\n",
    "\n",
    "from second.sphere.conv import init_depth_from_feature, depth_to_3D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"/home/yy/deeplearning/deeplearning/mypackages/second/configs/car.lite.nb.config\"\n",
    "config_path = \"/home/gx/GitHub/depconv/second/configs/car.fhd.config\"\n",
    "config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with open(config_path, \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, config)\n",
    "input_cfg = config.train_input_reader\n",
    "model_cfg = config.model.second\n",
    "# config_tool.change_detection_range(model_cfg, [-50, -50, 50, 50])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network, Target Assigner and Voxel Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_shape =  [1, 63, 512, 512, 16]\n",
      "[ 64 512 512]\n",
      "rpn debug, final_num_filters =  192\n",
      "rpn debug, num_cls =  2\n",
      "rpn debug, num_anchor_per_loc =  2\n",
      "feautres size [1, 64, 64]\n",
      "res = (1, 64, 64, 1, 2, 7)\n",
      "feautres size [1, 64, 64]\n",
      "res = (1, 64, 64, 1, 2, 7)\n",
      "remain number of infos: 3712\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"/home/gx/GitHub/depconv/second/depconv35/voxelnet-11600.tckpt\"\n",
    "net = build_network(model_cfg).to(device).eval()\n",
    "net.load_state_dict(torch.load(ckpt_path))\n",
    "target_assigner = net.target_assigner\n",
    "voxel_generator = net.voxel_generator\n",
    "\n",
    "dataset = input_reader_builder.build(\n",
    "    input_cfg,\n",
    "    model_cfg,\n",
    "    training=True,\n",
    "    voxel_generator=voxel_generator,\n",
    "    target_assigner=target_assigner,\n",
    "    multi_gpu=False)\n",
    "\n",
    "num_gpu=1\n",
    "collate_fn= merge_second_batch\n",
    "multi_gpu = False\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "#         num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "        num_workers=1,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "        worker_init_fn=_worker_init_fn,\n",
    "        drop_last=not multi_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feautres size [1, 64, 64]\n",
      "res = (1, 64, 64, 1, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "grid_size = voxel_generator.grid_size\n",
    "feature_map_size = grid_size[:2] // config_tool.get_downsample_factor(model_cfg)\n",
    "feature_map_size = [*feature_map_size, 1][::-1]\n",
    "\n",
    "anchors = target_assigner.generate_anchors(feature_map_size)[\"anchors\"]\n",
    "anchors = torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "anchors = anchors.view(1, -1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read KITTI infos\n",
    "you can load your custom point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_path = input_cfg.dataset.kitti_info_path\n",
    "root_path = Path(input_cfg.dataset.kitti_root_path)\n",
    "with open(info_path, 'rb') as f:\n",
    "    infos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Point Cloud, Generate Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = infos[564]\n",
    "v_path = info[\"point_cloud\"]['velodyne_path']\n",
    "v_path = str(root_path / v_path)\n",
    "points = np.fromfile(\n",
    "    v_path, dtype=np.float32, count=-1).reshape([-1, 4])\n",
    "# voxels, coords, num_points = voxel_generator.generate(points, max_voxels=90000)\n",
    "# print(voxels.shape)\n",
    "# add batch idx to coords\n",
    "# coords = np.pad(coords, ((0, 0), (1, 0)), mode='constant', constant_values=0)\n",
    "# voxels = torch.tensor(voxels, dtype=torch.float32, device=device)\n",
    "# coords = torch.tensor(coords, dtype=torch.int32, device=device)\n",
    "# num_points = torch.tensor(num_points, dtype=torch.int32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER 0 seed: 1590158259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gx/GitHub/depconv/second/core/geometry.py:272: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"points_in_convex_polygon_jit\" failed type inference due to: Invalid use of Function(<built-in function getitem>) with argument(s) of type(s): (array(float32, 3d, C), Tuple(slice<a:b>, list(int64), slice<a:b>))\n",
      " * parameterized\n",
      "In definition 0:\n",
      "    All templates rejected with literals.\n",
      "In definition 1:\n",
      "    All templates rejected without literals.\n",
      "In definition 2:\n",
      "    All templates rejected with literals.\n",
      "In definition 3:\n",
      "    All templates rejected without literals.\n",
      "In definition 4:\n",
      "    All templates rejected with literals.\n",
      "In definition 5:\n",
      "    All templates rejected without literals.\n",
      "In definition 6:\n",
      "    All templates rejected with literals.\n",
      "In definition 7:\n",
      "    All templates rejected without literals.\n",
      "In definition 8:\n",
      "    All templates rejected with literals.\n",
      "In definition 9:\n",
      "    All templates rejected without literals.\n",
      "In definition 10:\n",
      "    All templates rejected with literals.\n",
      "In definition 11:\n",
      "    All templates rejected without literals.\n",
      "In definition 12:\n",
      "    TypeError: unsupported array index type list(int64) in Tuple(slice<a:b>, list(int64), slice<a:b>)\n",
      "    raised from /home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/typing/arraydecl.py:69\n",
      "In definition 13:\n",
      "    TypeError: unsupported array index type list(int64) in Tuple(slice<a:b>, list(int64), slice<a:b>)\n",
      "    raised from /home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/typing/arraydecl.py:69\n",
      "In definition 14:\n",
      "    All templates rejected with literals.\n",
      "In definition 15:\n",
      "    All templates rejected without literals.\n",
      "This error is usually caused by passing an argument of a type that is unsupported by the named function.\n",
      "[1] During: typing of intrinsic-call at /home/gx/GitHub/depconv/second/core/geometry.py (288)\n",
      "\n",
      "File \"core/geometry.py\", line 288:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "        vec1 = polygon - polygon[:, [num_points_of_polygon - 1] +\n",
      "                                 list(range(num_points_of_polygon - 1)), :]\n",
      "                                 ^\n",
      "\n",
      "  @numba.jit\n",
      "/home/gx/GitHub/depconv/second/core/geometry.py:272: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"points_in_convex_polygon_jit\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"core/geometry.py\", line 296:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "    cross = 0.0\n",
      "    for i in range(num_points):\n",
      "    ^\n",
      "\n",
      "  @numba.jit\n",
      "/home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"points_in_convex_polygon_jit\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"core/geometry.py\", line 283:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "    # first convert polygon to directed lines\n",
      "    num_points_of_polygon = polygon.shape[1]\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"core/geometry.py\", line 283:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "    # first convert polygon to directed lines\n",
      "    num_points_of_polygon = polygon.shape[1]\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/home/gx/GitHub/depconv/second/core/geometry.py:272: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"points_in_convex_polygon_jit\" failed type inference due to: Invalid use of Function(<built-in function getitem>) with argument(s) of type(s): (array(float32, 3d, C), Tuple(slice<a:b>, list(int64), slice<a:b>))\n",
      " * parameterized\n",
      "In definition 0:\n",
      "    All templates rejected with literals.\n",
      "In definition 1:\n",
      "    All templates rejected without literals.\n",
      "In definition 2:\n",
      "    All templates rejected with literals.\n",
      "In definition 3:\n",
      "    All templates rejected without literals.\n",
      "In definition 4:\n",
      "    All templates rejected with literals.\n",
      "In definition 5:\n",
      "    All templates rejected without literals.\n",
      "In definition 6:\n",
      "    All templates rejected with literals.\n",
      "In definition 7:\n",
      "    All templates rejected without literals.\n",
      "In definition 8:\n",
      "    All templates rejected with literals.\n",
      "In definition 9:\n",
      "    All templates rejected without literals.\n",
      "In definition 10:\n",
      "    All templates rejected with literals.\n",
      "In definition 11:\n",
      "    All templates rejected without literals.\n",
      "In definition 12:\n",
      "    TypeError: unsupported array index type list(int64) in Tuple(slice<a:b>, list(int64), slice<a:b>)\n",
      "    raised from /home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/typing/arraydecl.py:69\n",
      "In definition 13:\n",
      "    TypeError: unsupported array index type list(int64) in Tuple(slice<a:b>, list(int64), slice<a:b>)\n",
      "    raised from /home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/typing/arraydecl.py:69\n",
      "In definition 14:\n",
      "    All templates rejected with literals.\n",
      "In definition 15:\n",
      "    All templates rejected without literals.\n",
      "This error is usually caused by passing an argument of a type that is unsupported by the named function.\n",
      "[1] During: typing of intrinsic-call at /home/gx/GitHub/depconv/second/core/geometry.py (288)\n",
      "\n",
      "File \"core/geometry.py\", line 288:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "        vec1 = polygon - polygon[:, [num_points_of_polygon - 1] +\n",
      "                                 list(range(num_points_of_polygon - 1)), :]\n",
      "                                 ^\n",
      "\n",
      "  @numba.jit\n",
      "/home/gx/GitHub/depconv/second/core/geometry.py:272: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"points_in_convex_polygon_jit\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"core/geometry.py\", line 296:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "    cross = 0.0\n",
      "    for i in range(num_points):\n",
      "    ^\n",
      "\n",
      "  @numba.jit\n",
      "/home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"points_in_convex_polygon_jit\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"core/geometry.py\", line 283:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "    # first convert polygon to directed lines\n",
      "    num_points_of_polygon = polygon.shape[1]\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/home/user/anaconda3/envs/depconv/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"core/geometry.py\", line 283:\n",
      "def points_in_convex_polygon_jit(points, polygon, clockwise=True):\n",
      "    <source elided>\n",
      "    # first convert polygon to directed lines\n",
      "    num_points_of_polygon = polygon.shape[1]\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    }
   ],
   "source": [
    "# example = {\n",
    "#     \"anchors\": anchors,\n",
    "#     \"voxels\": voxels,\n",
    "#     \"num_points\": num_points,\n",
    "#     \"coordinates\": coords,\n",
    "# }\n",
    "# pred = net(example)[0]\n",
    "examples = []\n",
    "loop_i = 0\n",
    "for example in dataloader:\n",
    "    loop_i += 1\n",
    "    if loop_i > 10:\n",
    "        break\n",
    "    examples.append(example)\n",
    "#     print(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17909, 1, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[1]['voxels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17463, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]['coordinates'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_features shape:  torch.Size([17415, 4])\n",
      "sparse_shape : [ 64 512 512]\n",
      "NCDHW =  1 64 3 64 64\n"
     ]
    }
   ],
   "source": [
    "example = examples[2]\n",
    "example = example_convert_to_torch(example, torch.float32)\n",
    "\n",
    "# with torch.no_grad():\n",
    "pred = net(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a168ef242545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcoors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coordinates\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_points\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size_dev' is not defined"
     ]
    }
   ],
   "source": [
    "voxels = example[\"voxels\"]\n",
    "num_points = example[\"num_points\"]\n",
    "coors = example[\"coordinates\"]\n",
    "num_points = example[\"num_points\"]\n",
    "preds_dict = net.network_forward(voxels, num_points, coors, batch_size_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[3]['box3d_lidar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = example['anchors']\n",
    "boxes\n",
    "boxes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(example['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(example['voxels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['anchors'][0][3027]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"points\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = voxel_generator.generate(\n",
    "                points, 20000)\n",
    "res['voxels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# boxes_lidar = boxes.detach().cpu().numpy()\n",
    "# boxes_ldar = pred[1]['box3d_lidar']\n",
    "# vis_voxel_size = [0.1, 0.1, 0.1]\n",
    "# vis_point_range = [-50, -30, -3, 50, 30, 1]\n",
    "# vis_point_range = [0, -40, -3, 80, 40, 1]\n",
    "# bev_map = simplevis.point_to_vis_bev(example[\"points\"], vis_voxel_size, vis_point_range)\n",
    "# bev_map = simplevis.draw_box_in_bev(bev_map, vis_point_range, boxes_lidar, [0, 255, 0], 2)\n",
    "# bev_map = simplevis.draw_box_in_bev(bev_map,vis_point_range, boxes_lidar[0][0:8000:80,:].cpu().numpy(), [0,255,0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(bev_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bev_map = simplevis.kitti_vis(example[\"points\"][0], boxes[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(bev_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = example_convert_to_torch(example, device=torch.device(\"cuda\"))\n",
    "\n",
    "\n",
    "voxels = example[\"voxels\"]\n",
    "num_points = example[\"num_points\"]\n",
    "coors = example[\"coordinates\"]\n",
    "batch_anchors = example[\"anchors\"]\n",
    "# feature = example[\"feature\"]\n",
    "batch_size_dev = batch_anchors.shape[0]\n",
    "voxel_features = net.voxel_feature_extractor(voxels, num_points,\n",
    "                                                      coors)\n",
    "spatial_features = net.middle_feature_extractor(\n",
    "            voxel_features, coors, batch_size_dev)\n",
    "# vfeatures = net.voxel_feature_extractor(voxels, num_points, coors)\n",
    "# preds_dict = net.network_forward(voxels, num_points, coors, batch_size_dev)\n",
    "# spatial_features = net.feature_extractor(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples[5]['voxels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "    plt.figure()\n",
    "    plt.imshow(spatial_features[0][i].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if feature.dim() == 3:\n",
    "    feature = feature.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    depth = init_depth_from_feature(feature, 512)\n",
    "\n",
    "depth = F.max_pool2d(depth, 3, padding=1, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_to_3D(feature, depth.long(), D=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth[0].detach().cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth[depth == 0 ] = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_bev(depth, R =500):\n",
    "    \"\"\"\n",
    "    convert depth to bev\n",
    "        depth: [H,W]\n",
    "        to, \n",
    "    \"\"\"\n",
    "    H, W = depth.shape\n",
    "\n",
    "    ret = np.zeros([R, W])\n",
    "    for r in range(H):\n",
    "        for i in range(W):\n",
    "            d = int(depth[r][i])\n",
    "            if d >= R:\n",
    "                d = R-1\n",
    "            ret[d][i] = 1\n",
    "    return ret.T\n",
    "\n",
    "plt.imshow(depth_bev(depth[0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth1 = F.max_pool2d(depth, (2,1), padding=0, stride=(1,2)) / 2 \n",
    "plt.imshow(depth_bev(depth1[0].detach().cpu().numpy(), 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth2 = F.max_pool2d(depth1, (2,1), padding=0, stride=(1,2))/2\n",
    "plt.imshow(depth_bev(depth2[0].detach().cpu().numpy(),  R=128))\n",
    "print(depth2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth3 = F.avg_pool2d(depth2, (2,1), padding=0, stride=(1,2))/2\n",
    "plt.imshow(depth_bev(depth3[0].detach().cpu().numpy(),  R=64))\n",
    "print(depth3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth4 = F.max_pool2d(depth3, (2,1), padding=0, stride=(1,2)) /2 \n",
    "plt.imshow(depth_bev(depth4[0].detach().cpu().numpy(), 32))\n",
    "print(depth4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth5 = F.max_pool2d(depth4, (2,1), padding=(0,0), stride=(1,2)) /2 \n",
    "plt.imshow(depth_bev(depth5[0].detach().cpu().numpy(), 16))\n",
    "print(depth5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev = torch.tensor(depth_bev(depth[0].detach().cpu().numpy())).unsqueeze(0)\n",
    "bev1 = F.max_pool2d(bev, 2, stride=2)\n",
    "plt.imshow(bev1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev2 = F.max_pool2d(bev1, 2, stride=2)\n",
    "plt.imshow(bev2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev3 = F.max_pool2d(bev2, 2, stride=2)\n",
    "plt.imshow(bev3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev4 = F.max_pool2d(bev3, 2, stride=2)\n",
    "plt.imshow(bev4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev5 = F.max_pool2d(bev4, 2, stride=2)\n",
    "plt.imshow(bev5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = net.feature_extractor.conv1(feature, depth)\n",
    "# x = net.feature_extractor.bn1(x)\n",
    "# x = F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(x[0][5].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_ = F.avg_pool2d(depth.float(), 2, padding=1, stride=(2,2)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth_[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in car_anchor_i:\n",
    "    x = i % 64\n",
    "    y = i // 64\n",
    "    print(x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = examples[5]\n",
    "# example = example_convert_to_torch(example, torch.float32)\n",
    "feature = example['feature']\n",
    "if feature.dim() == 3:\n",
    "    feature = feature.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    depth = init_depth_from_feature(feature, 512)\n",
    "    \n",
    "plt.figure(figsize = (25,25))\n",
    "plt.imshow(depth[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "car_anchor_i = []\n",
    "for i, t in enumerate(example['reg_targets'][0]):\n",
    "    if t.sum() > 0:\n",
    "        print(i, t)\n",
    "        car_anchor_i.append(i)\n",
    "# car_anchor_i\n",
    "\n",
    "car_anchors = example['anchors'][0][car_anchor_i]\n",
    "car_anchors\n",
    "\n",
    "vis_voxel_size = [0.01, 0.1, 0.1]\n",
    "vis_point_range = [0, -30, -3, 6, 30, 1]\n",
    "bev_map = simplevis.point_to_vis_bev(np.stack((np.log(r),points[:,1],points[:,2]), axis=1), vis_voxel_size, vis_point_range)\n",
    "\n",
    "# bev_map = simplevis.kitti_vis(example['points'][0],\n",
    "#                               car_anchors.cpu().numpy()  )\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(bev_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pnts = points\n",
    "pnts[3] = np.log(pnts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points[:,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.sqrt(points[:,0]**2 + points[:,1]**2 + points[:,2]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack((points[:,0],points[:,1], r), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda (depconv)",
   "language": "python",
   "name": "depconv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
